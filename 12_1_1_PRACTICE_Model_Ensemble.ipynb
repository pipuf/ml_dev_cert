{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pipuf/ml_dev_cert/blob/main/12_1_1_PRACTICE_Model_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning"
      ],
      "metadata": {
        "id": "Be2AElON6nDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble learning works by aggregating the predictions of a group of predictors, the results will be best than the best predictor alone, this group of predictors are called ensemble and the technique is called ensemble learning .\n",
        "\n",
        "For example we can train an ensemble of Decision Tree Classifiers each of then is trained on different subsets of the training dataset, and the class that has the most votes will be the final prediction to this ensemble of trees, this ensemble is called -> Random Forest Algorithm .\n",
        "\n",
        "Using independent predictors make the ensemble methods give better predictions, to get an independent predictors you can use different algorithms. Different algorithms will make different types of errors."
      ],
      "metadata": {
        "id": "-LAtLPLz6sNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as npdatasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "X2ZyIL_58hZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_wine()\n",
        "X = dataset.data\n",
        "y = dataset.target"
      ],
      "metadata": {
        "id": "MPqeCBGr-Nh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and testing dataset"
      ],
      "metadata": {
        "id": "VfRYtxmrAPQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply StandardScaler to dataset"
      ],
      "metadata": {
        "id": "1rt2Llo5-Xvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Voting Classifiers"
      ],
      "metadata": {
        "id": "jtqoFH_H6w9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voting method works by training some number predictors and the aggregate the predictions of each classifier, the final prediction is the class with the most votes, this method is called Hard Voting Classification ."
      ],
      "metadata": {
        "id": "ka6H3YqC6y2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "lr_classifier = LogisticRegression()\n",
        "rf_classifier = RandomForestClassifier()\n",
        "svc = SVC()\n",
        "\n",
        "# build VotingClassifier using these 3 classifiers"
      ],
      "metadata": {
        "id": "3nY_RQyt6rDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the new VotingClassifier"
      ],
      "metadata": {
        "id": "4ieh1GSdCdqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select a metric and print the result"
      ],
      "metadata": {
        "id": "jsdWwgJ3AuHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another type of voting is make the classifiers estimate the class probabilities and then averaging them over all the individual classifiers, the final prediction will be the highest class probability, this method is called Soft Voting Classification . In sklearn we need just to change the voting argument from 'hard' to ''soft'."
      ],
      "metadata": {
        "id": "BcuWtKP-62hV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Bagging and Pasting"
      ],
      "metadata": {
        "id": "_9CCKeoV66lJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the same classifier, but training them on different subsets of the training dataset, if the sampling was with replacement the method is called **bagging**, while if the sampling was without replacement the method is called **pasting** , the final prediction is made by aggregating the predictions of all predictors (most frequent prediction)."
      ],
      "metadata": {
        "id": "kXALTH7R69qG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# build BaggingClassifier using LogisticRegression"
      ],
      "metadata": {
        "id": "3VrAf6QQ68MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the new BaggingClassifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYaR0CF_CgPr",
        "outputId": "a5dd3605-9e06-48dc-a110-891cdcf3f788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=100)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select a metric and print the result"
      ],
      "metadata": {
        "id": "DMQCU3T1Cj3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can switch from bagging to pasting by changing the **bootstrap**argument from True to False.\n",
        "\n",
        "As we discussed that we can train the classifiers on different subsets of the training dataset, also we can train them on different subsets of the training dataset features using **max_features** and **bootstrap_features** arguments."
      ],
      "metadata": {
        "id": "d1RVZUvP7HNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Random Forests"
      ],
      "metadata": {
        "id": "PJ0LMnYY7HUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is an ensemble of Decision Tree Classifiers that trained using the bagging or the pasting methods. The Random Forest Classifier has hyperparameters of both Decision Tree Classifier and the bagging method, instead of searching for the best test at each node in the whole datasets like Decision Tree Classifiers, Random Forests search for the best test in a random subsets of the datasets which introduce more randomness."
      ],
      "metadata": {
        "id": "0a4IBQXp7Nx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# build a RandomForestClassifier"
      ],
      "metadata": {
        "id": "tvl8c-2D7L5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the new RandomForestClassifier"
      ],
      "metadata": {
        "id": "VUeMBd5fCpq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select a metric and print the result"
      ],
      "metadata": {
        "id": "L_kwkHxhCrCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Boosting"
      ],
      "metadata": {
        "id": "2ITIvW1x7QKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting ensemble methods works by training the predictors sequentially, each predictor try to correct the one before it."
      ],
      "metadata": {
        "id": "FtJd6NL67Ub6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 AdaBoost"
      ],
      "metadata": {
        "id": "1oWMSQM27Xb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One if the most popular Boosting ensemble methods is the AdaBoost , each predictor will give more attention to the predecessor underfit, by increasing the relative weights of the mislabeled instances, and this updated weights will be used by the next predictor for training and prediction. This process will be repeated until the last predictor."
      ],
      "metadata": {
        "id": "VDMy7nVV7ad2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# build a AdaBoostClassifier using LogisticRegression"
      ],
      "metadata": {
        "id": "UMupQFlT7Scg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the new AdaBoostClassifier"
      ],
      "metadata": {
        "id": "SrC4t6UNDGR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select a metric and print the result"
      ],
      "metadata": {
        "id": "j-uGeAasDGZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Gradient Boosting"
      ],
      "metadata": {
        "id": "OKVXXXnW7ceZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like the AdaBoost the predictors are trained sequentially each predictor try to correct the one before it, but rather that updating the weights Gradient Boosting fit the new predictor to the residual errors that made by the previous predictor."
      ],
      "metadata": {
        "id": "jcBIy9td7cws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# build a GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "oFUxal0p7gsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the new GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "USUlNw5LDPmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select a metric and print the result"
      ],
      "metadata": {
        "id": "sQHIr5jSDPtz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}