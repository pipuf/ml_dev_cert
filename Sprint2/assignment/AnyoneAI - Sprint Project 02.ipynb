{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d3c07ee-c3bf-4530-812a-36405502b38d",
   "metadata": {
    "id": "9d3c07ee-c3bf-4530-812a-36405502b38d"
   },
   "source": [
    "# AnyoneAI - Sprint Project 02\n",
    "> Home Credit Default Risk\n",
    "\n",
    "You've been learning a lot about Machine Learning Algorithms, now we you're gonna be asked to put it all togheter. \n",
    "\n",
    "You will create a complete pipeline to preprocess the data, train your model and then predict values for the [Home Credit Default Risk](https://www.kaggle.com/competitions/home-credit-default-risk/) Kaggle competition.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e396c950-04b0-453e-b930-a22a96cee2d1",
   "metadata": {
    "id": "e396c950-04b0-453e-b930-a22a96cee2d1"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This is a binary Classification task: we want to predict whether the person applying for a home credit will be able to repay their debt or not. Our model will have to predict a 1 indicating the client will have payment difficulties: he/she will have late payment of more than X days on at least one of the first Y installments of the loan in our sample, 0 in all other cases.\n",
    "\n",
    "The dataset is composed of multiple files with different information about loans taken. In this project, we will work exclusively with the primary files: `application_train_aai.csv` and `application_test_aai.csv`.\n",
    "\n",
    "We will use [Area Under the ROC Curve](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl=es_419) as the evaluation metric, so our models will have to return the probabilities that a loan is not paid for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OzQjTwlkUT0C",
   "metadata": {
    "executionInfo": {
     "elapsed": 2252,
     "status": "ok",
     "timestamp": 1670194396248,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "OzQjTwlkUT0C"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src import config, data_utils, preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ab085b5-379c-4e02-9f96-08edf5dbf887",
   "metadata": {
    "id": "3ab085b5-379c-4e02-9f96-08edf5dbf887"
   },
   "source": [
    "### Getting the data\n",
    "\n",
    "To access the data for this project, you only need to execute the code below. This will download three files inside the `dataset` folder:\n",
    "\n",
    "- `application_train_aai.csv`: Training dataset you must use to train and find the best hyperparameters on your model.\n",
    "\n",
    "- `application_test_aai.csv`: Test dataset without labels. Because of the absence of labels, you can't use this dataset for your experiments. You will use the file only at the end after you choose what you think is the best model for the tasks. You will have to use that model to fill values in the `TARGET` column using the model predictions. Then submit this dataset alongside this Jupyter notebook, AnyoneAI will internally evaluate your model's accuracy in the hidden data and communicate later ;).\n",
    "\n",
    "- `HomeCredit_columns_description.csv`: This file contains descriptions for the columns in train and test datasets.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25a0a724-ceb1-40cb-b123-b8c907a9c06f",
   "metadata": {
    "id": "25a0a724-ceb1-40cb-b123-b8c907a9c06f"
   },
   "source": [
    "1.1. Load the training and test datasets. Also, the auxiliary file `HomeCredit_columns_description.csv` has additional information about the features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MnA4l8-rWraC",
   "metadata": {
    "id": "MnA4l8-rWraC"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jrkLdOJnWoSS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1670195316027,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "jrkLdOJnWoSS",
    "outputId": "ce9f5ee1-6ed0-4b6f-d8f5-37d38b4e4773"
   },
   "outputs": [],
   "source": [
    "app_train, app_test, columns_description = data_utils.get_datasets()\n",
    "\n",
    "\n",
    "if app_train.shape == (246008, 122):\n",
    "    print(\"Success: app_train shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(app_train, pd.DataFrame):\n",
    "    print(\"Success: app_train type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train dataset type is incorrect, please review your code\")\n",
    "\n",
    "if app_test.shape == (61503, 122):\n",
    "    print(\"Success: app_test shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(app_test, pd.DataFrame):\n",
    "    print(\"Success: app_test type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset type is incorrect, please review your code\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c10a608-5c38-44f4-8158-18225619e7ae",
   "metadata": {
    "id": "7c10a608-5c38-44f4-8158-18225619e7ae",
    "tags": []
   },
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "A lot of the analysis of the data can be found on publicly available Kaggle kernels or blog posts, but you need to make sure you understand the dataset's properties before starting working on it, so we'll do exploratory data analysis for the main files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ea774d3-e12c-4c2b-846d-8c5c03e70928",
   "metadata": {
    "id": "2ea774d3-e12c-4c2b-846d-8c5c03e70928"
   },
   "source": [
    "#### Dataset Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b4817-5768-49d2-b5f2-907827541f16",
   "metadata": {
    "id": "9f0b4817-5768-49d2-b5f2-907827541f16"
   },
   "source": [
    "1.2. Print how many samples do we have in our train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a98fdd5-cbf2-4d20-9559-89c7cf5943cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1670195319978,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "6a98fdd5-cbf2-4d20-9559-89c7cf5943cd",
    "outputId": "bec830ab-d760-4019-d277-d4249be2948d"
   },
   "outputs": [],
   "source": [
    "### TODO Complete in this cell: shape of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e7a95b-8288-4ead-8c1e-f2cf68167d8e",
   "metadata": {
    "id": "74e7a95b-8288-4ead-8c1e-f2cf68167d8e"
   },
   "source": [
    "1.3. List all columns in the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af2f2e-93db-41e6-bb26-df1ad0be7786",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1670195328520,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "d7af2f2e-93db-41e6-bb26-df1ad0be7786",
    "outputId": "cc0cbeb7-edf1-4d3a-cb05-3341232c2606"
   },
   "outputs": [],
   "source": [
    "### TODO Complete in this cell: Show all columns in the training dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc4835f-90c3-4049-9ec2-44d102201104",
   "metadata": {
    "id": "2dc4835f-90c3-4049-9ec2-44d102201104"
   },
   "source": [
    "1.4. Show the first 5 records of the training dataset, transpose the dataframe to see each record as a column and features as rows, make sure all features are visualized. Take your time to review what kind of information you can gather from this data.\n",
    "\n",
    "For reference only, it should look like this:\n",
    "\n",
    "|0|1|2|3|4\n",
    "|---|---|---|---|---\n",
    "Unnamed: 0|187399|84777|268140|270686|33785\n",
    "SK_ID_CURR|317244|198357|410700|413785|139141\n",
    "TARGET|0|0|0|0|0\n",
    "NAME_CONTRACT_TYPE|Cash loans|Cash loans|Cash loans|Cash loans|Cash loans\n",
    "...|...|...|...|...|...\n",
    "AMT_REQ_CREDIT_BUREAU_DAY|0.0|0.0|0.0|0.0|0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c85b3-52e0-4b80-9753-afb81f92bd0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1670195332530,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "ac9c85b3-52e0-4b80-9753-afb81f92bd0d",
    "outputId": "2d56774f-18a2-48f2-f4b4-da9decca6012"
   },
   "outputs": [],
   "source": [
    "### TODO Complete in this cell: show first 5 records in a transposed table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da8f46-e503-4b67-8e73-ac61c394824c",
   "metadata": {
    "id": "59da8f46-e503-4b67-8e73-ac61c394824c"
   },
   "source": [
    "1.5. Show the distribution of the target variable values: print the total value count and the percentage of each value, plot this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855d370-c825-415b-9dd0-9dbdd576fada",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1670195337134,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "8855d370-c825-415b-9dd0-9dbdd576fada",
    "outputId": "13794aaf-d179-4506-f087-ad8cd0be3e98"
   },
   "outputs": [],
   "source": [
    "### TODO Complete in this cell: show distribution of target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b5a63-da31-4a61-9e91-9708dc7633a7",
   "metadata": {
    "id": "cc7b5a63-da31-4a61-9e91-9708dc7633a7"
   },
   "source": [
    "1.6. Show the number of columns of each data type.\n",
    "\n",
    "Just for giving you an idea, the output should look like this (not exactly the same numbers):\n",
    "\n",
    "```python\n",
    "float64    45\n",
    "int64      32\n",
    "object     10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74878dd-cc48-4e69-bc35-e90457d54b3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1666034867408,
     "user": {
      "displayName": "Federico Morales",
      "userId": "06983145799989655383"
     },
     "user_tz": 180
    },
    "id": "b74878dd-cc48-4e69-bc35-e90457d54b3a",
    "outputId": "5c188262-b9b1-4368-b952-fd2b5608a864"
   },
   "outputs": [],
   "source": [
    "### TODO Complete in this cell: show number of columns per data type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a8799e-d32a-4239-a85d-6ff29ab4682f",
   "metadata": {
    "id": "c0a8799e-d32a-4239-a85d-6ff29ab4682f"
   },
   "source": [
    "1.7. For categorical variables (`object` data type), show the number of distinct values in each column (number of labels).\n",
    "\n",
    "Just for giving you an idea, the output should look like this (not exactly the same numbers):\n",
    "\n",
    "```python\n",
    "NAME_CONTRACT_TYPE             5\n",
    "CODE_GENDER                    2\n",
    "FLAG_OWN_CAR                   1\n",
    "FLAG_OWN_REALTY                1\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b294976-dc0d-44bd-9bf6-29ba1f6a2e2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1666034867708,
     "user": {
      "displayName": "Federico Morales",
      "userId": "06983145799989655383"
     },
     "user_tz": 180
    },
    "id": "8b294976-dc0d-44bd-9bf6-29ba1f6a2e2b",
    "outputId": "277be5a9-e286-4d82-956d-7af49e6112ec"
   },
   "outputs": [],
   "source": [
    "### TODO Complete in this cell: show number of unique values per categorical column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967d1938-e27d-4fbd-9bdc-f26364f5fdfd",
   "metadata": {
    "id": "967d1938-e27d-4fbd-9bdc-f26364f5fdfd"
   },
   "source": [
    "1.8. Analyzing missing data: show the percentage of missing data for each column ordered by percentage descending (show only the 20 columns with higher missing pct)\n",
    "\n",
    "Just for giving you an idea, the output should look like this (not exactly the same numbers and columns names):\n",
    "\n",
    "```python\n",
    "                   Total   Percent\n",
    "COMMONAREA_AVG    121000      85.2\n",
    "COMMONAREA_MODE   121000      76.6\n",
    "COMMONAREA_MEDI   121000      62.9\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b65aa7e-d8a2-44b0-9803-ca5277471470",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1666034868311,
     "user": {
      "displayName": "Federico Morales",
      "userId": "06983145799989655383"
     },
     "user_tz": 180
    },
    "id": "0b65aa7e-d8a2-44b0-9803-ca5277471470",
    "outputId": "dca74fc2-8fd7-4d29-ef67-f58c3abf2e09",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TODO Complete in this cell: checking missing data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "783c5bb6-6b6d-4e34-bbe2-3a5dc319a4f0",
   "metadata": {
    "id": "783c5bb6-6b6d-4e34-bbe2-3a5dc319a4f0"
   },
   "source": [
    "#### Analyzing distribution of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79d319-bde8-47a4-890d-6694edf221e2",
   "metadata": {
    "id": "9c79d319-bde8-47a4-890d-6694edf221e2"
   },
   "source": [
    "1.9. Show the distribution of credit amounts.\n",
    "\n",
    "*Hint:* Take a look at `AMT_CREDIT` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc953042-9744-498e-a435-fa660e76c70d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 2137,
     "status": "ok",
     "timestamp": 1666034870446,
     "user": {
      "displayName": "Federico Morales",
      "userId": "06983145799989655383"
     },
     "user_tz": 180
    },
    "id": "bc953042-9744-498e-a435-fa660e76c70d",
    "outputId": "28e9ab9e-788b-4ca8-e796-a672103d6045",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TODO Complete in this cell: distribution of credit amounts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7bf4a-25c7-47c0-aea8-ed916d8827f6",
   "metadata": {
    "id": "f4e7bf4a-25c7-47c0-aea8-ed916d8827f6"
   },
   "source": [
    "1.10. Plot the education level of the credit applicants, show the percentages of each category. Also print the total counts for each category.\n",
    "\n",
    "*Hint:* Take a look at `NAME_EDUCATION_TYPE` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d7d06-b21a-4938-8e3d-11798def489b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666034870446,
     "user": {
      "displayName": "Federico Morales",
      "userId": "06983145799989655383"
     },
     "user_tz": 180
    },
    "id": "406d7d06-b21a-4938-8e3d-11798def489b",
    "outputId": "41fa858b-e391-427a-9e2d-b21c97ec0993"
   },
   "outputs": [],
   "source": [
    "### TODO Complete in this cell: level of education plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e17d30-acce-4b7f-bacc-af488cda3e7d",
   "metadata": {
    "id": "78e17d30-acce-4b7f-bacc-af488cda3e7d"
   },
   "source": [
    "1.11. Plot the distribution of ocupation of the loan applicants.\n",
    "\n",
    "*Hint:* Take a look at `OCCUPATION_TYPE` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec101d97-c77b-4e5d-b69f-7c2f8f652d8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1666034871130,
     "user": {
      "displayName": "Federico Morales",
      "userId": "06983145799989655383"
     },
     "user_tz": 180
    },
    "id": "ec101d97-c77b-4e5d-b69f-7c2f8f652d8f",
    "outputId": "50aee514-131b-47ed-dea7-0d4de24d5c6a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TODO Complete in this cell: ocupation of applicants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b1bc0-4dad-4ce8-b574-cbae5984a589",
   "metadata": {
    "id": "824b1bc0-4dad-4ce8-b574-cbae5984a589"
   },
   "source": [
    "1.12. Plot the family status of the applicants.\n",
    "\n",
    "*Hint:* Take a look at `NAME_FAMILY_STATUS` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95463328-4392-435e-a203-95e28998930b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666034871131,
     "user": {
      "displayName": "Federico Morales",
      "userId": "06983145799989655383"
     },
     "user_tz": 180
    },
    "id": "95463328-4392-435e-a203-95e28998930b",
    "outputId": "b19a8a5c-32e4-4408-e6c2-7b37beb52061"
   },
   "outputs": [],
   "source": [
    "### TODO Complete in this cell: family status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa77db6-a50a-454e-b1b2-83abe3c8baae",
   "metadata": {
    "id": "efa77db6-a50a-454e-b1b2-83abe3c8baae"
   },
   "source": [
    "1.13. Plot the income type of applicants grouped by the target variable.\n",
    "\n",
    "*Hint:* Take a look at `NAME_INCOME_TYPE` and `TARGET` columns. You can use `hue` parameter on Seaborn to group samples using another categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c3267-56c8-45fc-9184-b822cd026571",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "executionInfo": {
     "elapsed": 3107,
     "status": "ok",
     "timestamp": 1666034874234,
     "user": {
      "displayName": "Federico Morales",
      "userId": "06983145799989655383"
     },
     "user_tz": 180
    },
    "id": "f22c3267-56c8-45fc-9184-b822cd026571",
    "outputId": "149ed35d-aa89-4988-8131-7b58a395d7f4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TODO Complete in this cell: Income type of applicants by target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b1ae3-c943-4737-bdb8-2ecff733b27b",
   "metadata": {
    "id": "675b1ae3-c943-4737-bdb8-2ecff733b27b"
   },
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "VvT_6ZEOztAQ",
   "metadata": {
    "id": "VvT_6ZEOztAQ"
   },
   "source": [
    "2.1. The next step will be to separate our train and test datasets columns between Features (the input to the model) and Targets (what the model has to predict with the given features).\n",
    "\n",
    "- Assign to `X_train` all the columns from `app_train` that should be used as features for training our models.\n",
    "- Assign to `y_train` the single column from `app_train` that should be used as our target (i.e. what we want to predict).\n",
    "- Assign to `X_test` all the columns from `app_test` that should be used as features for training our models.\n",
    "- Assign to `y_test` the single column from `app_test` that should be used as our target (i.e. what we want to predict).\n",
    "\n",
    "To do that, you will have to complete the function `data_utils.get_feature_target()` in all the parts with a `TODO` mark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrkoywq9aNvJ",
   "metadata": {
    "id": "wrkoywq9aNvJ"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5F5UeGj1aNvJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 511,
     "status": "ok",
     "timestamp": 1670195363853,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "5F5UeGj1aNvJ",
    "outputId": "6ce8ac80-c09c-43dd-a537-cc7b25220efe"
   },
   "outputs": [],
   "source": [
    "# Now we execute the function above to get the result\n",
    "X_train, y_train, X_test, y_test = data_utils.get_feature_target(app_train, app_test)\n",
    "\n",
    "\n",
    "if X_train.shape == (246008, 121):\n",
    "    print(\"Success: X_train shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"X_train dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    print(\"Success: X_train type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train dataset type is incorrect, please review your code\")\n",
    "\n",
    "if y_train.shape == (246008,) or y_train.shape == (246008, 1):\n",
    "    print(\"Success: y_train shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train labels shape is incorrect, please review your code\")\n",
    "\n",
    "if X_test.shape == (61503, 121):\n",
    "    print(\"Success: X_test shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(X_test, pd.DataFrame):\n",
    "    print(\"Success: X_test type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset type is incorrect, please review your code\")\n",
    "\n",
    "if y_test.shape == (61503,) or y_test.shape == (61503, 1):\n",
    "    print(\"Success: y_test shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test labels shape is incorrect, please review your code\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46078a39",
   "metadata": {},
   "source": [
    "2.2. In order to avoid overfitting while searching for the best model hyperparameters, it's always a good idea to split our training dataset into two new sets called `train` and `validation`. \n",
    "\n",
    "While the `train` data will be used to fit the model and adjust its internal weights, the `validation` will be exclusively used to test the model performance on unseen data during training, it's like a testing dataset used during experimentation.\n",
    "\n",
    "Remember we can't use the `test` dataset to validate the model performance because this one lacks of labels :( So the `validation` data will be the only resource you will have to evaluate the final model performance before doing your submission.\n",
    "\n",
    "To do that, you will have to complete the function `data_utils.get_train_val_sets()` in all the parts with a `TODO` mark.\n",
    "\n",
    "This function should perform these activities:\n",
    "- Use the `sklearn.model_selection.train_test_split` function with `X_train`, `y_train` datasets.\n",
    "- Assign only 20% of the dataset for testing (see `test_size` parameter in `train_test_split`)\n",
    "- Assign a seed so we get reproducible output across multiple function calls (see `random_state` parameter in `train_test_split`)\n",
    "- Shuffle the data (see `shuffle` parameter in `train_test_split`)\n",
    "\n",
    "For reference, see:\n",
    "- [Scikit-learn train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "- [Wikipedia: Training, validation, and test data sets](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)\n",
    "- [Train Test Validation Split: How To & Best Practices](https://www.v7labs.com/blog/train-validation-test-set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec771222",
   "metadata": {},
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d31b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we execute the function above to get the result\n",
    "X_train, X_val, y_train, y_val = data_utils.get_train_val_sets(X_train, y_train)\n",
    "\n",
    "\n",
    "if X_train.shape == (196806, 121):\n",
    "    print(\"Success: X_train shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"X_train dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    print(\"Success: X_train type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train dataset type is incorrect, please review your code\")\n",
    "\n",
    "if y_train.shape == (196806,) or y_train.shape == (196806, 1):\n",
    "    print(\"Success: y_train shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train labels shape is incorrect, please review your code\")\n",
    "\n",
    "if X_val.shape == (49202, 121):\n",
    "    print(\"Success: X_test shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(X_val, pd.DataFrame):\n",
    "    print(\"Success: X_test type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset type is incorrect, please review your code\")\n",
    "\n",
    "if y_val.shape == (49202,) or y_val.shape == (49202, 1):\n",
    "    print(\"Success: y_test shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test labels shape is incorrect, please review your code\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51397c38-0204-454b-8fe6-011dc8c38418",
   "metadata": {
    "id": "51397c38-0204-454b-8fe6-011dc8c38418"
   },
   "source": [
    "2.3. In this section, you will code a function to make all the data pre-processing for the dataset. What you have to deliver is a function that takes `X_train`, `X_val`, and `X_test` dataframes, processes all features, and returns the transformed data as numpy arrays ready to be used for training.\n",
    "\n",
    "The function should perform these activities, in this order:\n",
    "\n",
    "1. Correct outliers/anomalous values in numerical columns (`DAYS_EMPLOYED` column)\n",
    "2. Encode string categorical features (dytpe `object`):\n",
    "    - If the feature has 2 categories encode using binary encoding\n",
    "    - More than 2 categories, use one hot encoding \n",
    "3. Impute values for all columns with missing data (use median as imputing value)\n",
    "4. Feature scaling with Min-Max scaler.\n",
    "\n",
    "Complete the function `preprocessing.preprocess_data()` following the instructions given above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "-1YXG39vc2qX",
   "metadata": {
    "id": "-1YXG39vc2qX"
   },
   "source": [
    "In the following cell, we are going to execute the preprocessing function you've just coded. No need to modify this.\n",
    "\n",
    "**Important Note:** From now on, you must always use `train_data` for training your models and `val_data` only for the final evaluation of the model trained. About `test_data`, it will be used at the end only for submitting your final model predictions and be evaluated on our side with the hidden annotations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mMkqwi0gd7a8",
   "metadata": {
    "id": "mMkqwi0gd7a8"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whWcb5jtcyYe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1670195395495,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "whWcb5jtcyYe",
    "outputId": "5bb07f0e-a0b6-4773-94b1-97c5f2ccd053"
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = preprocessing.preprocess_data(X_train, X_val, X_test)\n",
    "\n",
    "\n",
    "if train_data.shape == (196806, 246):\n",
    "    print(\"Success: train_data shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"train_data dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(train_data, np.ndarray):\n",
    "    print(\"Success: train_data type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train dataset type is incorrect, please review your code\")\n",
    "\n",
    "if val_data.shape == (49202, 246):\n",
    "    print(\"Success: val_data shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"val_data dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(val_data, np.ndarray):\n",
    "    print(\"Success: val_data type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Validation dataset type is incorrect, please review your code\")\n",
    "\n",
    "if test_data.shape == (61503, 246):\n",
    "    print(\"Success: test_data shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"test_data dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(test_data, np.ndarray):\n",
    "    print(\"Success: test_data type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset type is incorrect, please review your code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5df1e8-1f03-4f14-9dbd-292d3b84859d",
   "metadata": {
    "id": "2f5df1e8-1f03-4f14-9dbd-292d3b84859d"
   },
   "source": [
    "## 3. Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83062f02-7157-4083-b57b-78fbc95fe39b",
   "metadata": {
    "id": "83062f02-7157-4083-b57b-78fbc95fe39b"
   },
   "source": [
    "As usual, you will start training simple models and will progressively move to more complex models and pipelines.\n",
    "\n",
    "**Pro tip:** It is of utmost importance to make an accurate estimation of the time required to train a machine learning model. Because of this, we recommend you to use Python [time](https://docs.python.org/3/library/time.html) library or Jupyter magic function `%%time` on the cell you're training your model to get an estimate of the time it took to fit your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99a786-a17e-4caa-9ac7-48fced40fa71",
   "metadata": {
    "id": "7e99a786-a17e-4caa-9ac7-48fced40fa71"
   },
   "source": [
    "### Baseline: LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e10796-6d86-4fce-8738-4b9c4fbbf359",
   "metadata": {
    "id": "91e10796-6d86-4fce-8738-4b9c4fbbf359"
   },
   "source": [
    "3.1. Import LogisticRegression from sklearn and train a model using the preprocesed train data from the previous section, and just default parameters. If you receive a warning because the algorithm failed to converge, try increasing the number of iterations or decreasing the C parameter.\n",
    "\n",
    "Assign the trained model to `log_reg` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2OTXvqibhqid",
   "metadata": {
    "id": "2OTXvqibhqid"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This is an example code on how to:\n",
    "#   - Create and fit (train) a logistic regression\n",
    "#   - Assign to `log_reg` variable\n",
    "log_reg = None\n",
    "log_reg = LogisticRegression(C=0.0001)\n",
    "log_reg.fit(train_data, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2OTXvqibhqid",
   "metadata": {
    "id": "2OTXvqibhqid"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JmFsb5DShqid",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1670195424262,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "JmFsb5DShqid",
    "outputId": "3cc332c0-18c0-4257-82b7-3e95e05bc012"
   },
   "outputs": [],
   "source": [
    "if isinstance(log_reg, LogisticRegression):\n",
    "    print(\"Success: Logistic regression model type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Logistic regression model type is incorrect, please review your code\"\n",
    "    )\n",
    "\n",
    "check_is_fitted(log_reg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "018d5c7e-012f-47cc-8bfc-0332de907ae9",
   "metadata": {
    "id": "018d5c7e-012f-47cc-8bfc-0332de907ae9"
   },
   "source": [
    "3.2. Use the trained model to predict probabilities for `train_data` and `val_data`.\n",
    "\n",
    "**Important note:** When using the function `predict_proba()` for getting model probabilities you will get, for each sample, a tuple indicating the probability for class 0 and for class 1 respectively. For computing the AUC ROC score we only need the probability that the debt is not repaid (equivalent to class 1). As an example, the result from running `predict_proba()` on validation dataset will have a shape of `(49202, 2)` but, we only need the second column from that matrix, which corresponds to the class 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2OTXvqibhqid",
   "metadata": {
    "id": "2OTXvqibhqid"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ae69b-1fe2-458d-9de7-3e1ed70b9b02",
   "metadata": {
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1670195428153,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "798ae69b-1fe2-458d-9de7-3e1ed70b9b02"
   },
   "outputs": [],
   "source": [
    "# Example code to show you how to use the Logistic Regression model\n",
    "# to predict probabilities for each class and then, use the probabilities for the\n",
    "# class 1 only.\n",
    "\n",
    "# Train data predictions (class 1)\n",
    "log_reg_train = log_reg.predict_proba(train_data)[:, 1]\n",
    "\n",
    "# Validation data predictions (class 1)\n",
    "log_reg_val = log_reg.predict_proba(val_data)[:, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0826305-4900-4ba1-bf25-48205be980c6",
   "metadata": {
    "id": "b0826305-4900-4ba1-bf25-48205be980c6"
   },
   "source": [
    "3.3. Get AUC ROC score on train and validation datasets. See [scikit-learn AUC ROC function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) for a working implementation.\n",
    "\n",
    "Assign the AUC ROC score to `lr_roc_auc` variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2OTXvqibhqid",
   "metadata": {
    "id": "2OTXvqibhqid"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a850c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to show you how to get the ROC AUC Score on train and val datasets\n",
    "\n",
    "# Train ROC AUC Score\n",
    "roc_auc_train = roc_auc_score(y_true=y_train, y_score=log_reg_train)\n",
    "print(f\"Train ROC AUC Score: {roc_auc_train:.4f}\")\n",
    "\n",
    "# Validation ROC AUC Score\n",
    "roc_auc_val = roc_auc_score(y_true=y_val, y_score=log_reg_val)\n",
    "print(f\"Validation ROC AUC Score: {roc_auc_val:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d889582-0c21-4789-acac-4d58f8eb74d9",
   "metadata": {
    "id": "4d889582-0c21-4789-acac-4d58f8eb74d9"
   },
   "source": [
    "At this point, the model should produce a result of around 0.67.\n",
    "\n",
    "**Question:** Comparing train and validation results, do you observe underfitting, overfitting, or none of those two?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ap_JpPoNidg2",
   "metadata": {
    "id": "ap_JpPoNidg2"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KWSHN2Ouidg2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1670195435233,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "KWSHN2Ouidg2",
    "outputId": "03f1bf8e-8980-4eba-d9e8-aa53c5dccd84"
   },
   "outputs": [],
   "source": [
    "if isinstance(roc_auc_val, float):\n",
    "    print(\"Success: AUC ROC score type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"AUC ROC score type is incorrect, please review your code\")\n",
    "\n",
    "if roc_auc_val >= 0.6:\n",
    "    print(\"Success: AUC ROC score is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"AUC ROC score is incorrect, please review your code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8300cc-d8be-4303-9042-757cb9e15d3e",
   "metadata": {
    "id": "8f8300cc-d8be-4303-9042-757cb9e15d3e"
   },
   "source": [
    "### Training a Random Forest Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05280b49-11af-4fe6-9236-95f31fb5e0d5",
   "metadata": {
    "id": "05280b49-11af-4fe6-9236-95f31fb5e0d5"
   },
   "source": [
    "You're gonna start working in more complex models: ensambles, particularly, you're going to use the Random Forest Classifier from Scikit Learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efdfd67-ec68-49b3-8727-7ab9784b5e54",
   "metadata": {
    "id": "0efdfd67-ec68-49b3-8727-7ab9784b5e54"
   },
   "source": [
    "3.4. Train a RandomForestClassifier, print the time taken by the fit function. Just use default hyperparameters, except for `n_jobs`, which should be set to \"-1\" to allow the library to use all CPU cores to speed up training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b9ea6-4bf9-42f0-aed1-1c0f3a4f9b39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95269,
     "status": "ok",
     "timestamp": 1670195539807,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "1d3b9ea6-4bf9-42f0-aed1-1c0f3a4f9b39",
    "outputId": "848010ac-1f2b-43ab-9873-0d78c186e804"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO Write your code here for training a Random Forest model.\n",
    "#   - Please use sklearn.ensemble.RandomForestClassifier() class.\n",
    "#   - Assign the model to the variable `rf`.\n",
    "#   - Remember to fit the model only on `train_data`.\n",
    "rf = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bf912",
   "metadata": {
    "id": "2OTXvqibhqid"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21b91c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1670195424262,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "JmFsb5DShqid",
    "outputId": "3cc332c0-18c0-4257-82b7-3e95e05bc012"
   },
   "outputs": [],
   "source": [
    "if isinstance(rf, RandomForestClassifier):\n",
    "    print(\"Success: RandomForestClassifier model type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"RandomForestClassifier model type is incorrect, please review your code\"\n",
    "    )\n",
    "\n",
    "check_is_fitted(rf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b12a0a5-8ff1-4cb6-9928-37122d2a3435",
   "metadata": {
    "id": "3b12a0a5-8ff1-4cb6-9928-37122d2a3435"
   },
   "source": [
    "3.5. Use the classifier to predict probabilities for `train_data` and `val_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba80108-858c-4d4e-ba19-7f19fa526dc9",
   "metadata": {
    "executionInfo": {
     "elapsed": 4992,
     "status": "ok",
     "timestamp": 1670195577761,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "4ba80108-858c-4d4e-ba19-7f19fa526dc9"
   },
   "outputs": [],
   "source": [
    "# TODO Use the Random Forest model to predict probabilities for each class and then,\n",
    "# use the probabilities for the class 1 only.\n",
    "\n",
    "# Train data predictions (class 1)\n",
    "rf_pred_train = \n",
    "\n",
    "# Validation data predictions (class 1)\n",
    "rf_pred_val = "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "W3TrmlUF2pDM",
   "metadata": {
    "id": "W3TrmlUF2pDM"
   },
   "source": [
    "3.6. Get AUC ROC score on train and validation datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D4jft3Sw2pDN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1670195702088,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "D4jft3Sw2pDN",
    "outputId": "74877c11-92b3-43d3-afbe-08fb51201708"
   },
   "outputs": [],
   "source": [
    "# TODO Get the ROC AUC Score on train_data and val_data datasets.\n",
    "# Train ROC AUC Score\n",
    "roc_auc_train = \n",
    "\n",
    "# Validation ROC AUC Score\n",
    "roc_auc_val = "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8_buAhkG24ZC",
   "metadata": {
    "id": "8_buAhkG24ZC"
   },
   "source": [
    "At this point, the model should produce a result around 0.7.\n",
    "\n",
    "**Question:** Comparing train and validation results, do you observe underfitting, overfitting, or none of those two?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acfcc24-62b6-4118-9c42-a0268dcf5c53",
   "metadata": {
    "id": "4acfcc24-62b6-4118-9c42-a0268dcf5c53"
   },
   "source": [
    "### Randomized Search with Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd634b2b-68af-4db1-b062-1496f8d1179e",
   "metadata": {
    "id": "dd634b2b-68af-4db1-b062-1496f8d1179e"
   },
   "source": [
    "So far, we've only created models using the default hyperparameters of each algorithm. This is usually something that we would only do for baseline models, hyperparameter tuning is a very important part of the modeling process and is often the difference between having an acceptable model or not.\n",
    "\n",
    "But, there are usually lots of hyperparameters to tune and a finite amount of time to do it, you have to consider the time and resources it takes to find an optimal combination of them. In the previous section you trained a random forest classifier and saw how much it took to train it once in your PC. If you want to do hyperparameter optimization you now have to consider that you will have to train the algorithm N number of times, with N being the cartesian product of all parameters. \n",
    "\n",
    "Furthermore, you can't validate the performance of your trained models on the test set, as this data should only be used to validate the final model. So we have to implement a validation strategy, K-Fold Cross Validation being the most common. But this also adds time complexity to our training, because we will have to train each combinations of hyperparameters M number of times, X being the number of folds in which we divided our dataset, so the total number of training iterations will be NxM... this resulting number can grow VERY quickly.\n",
    "\n",
    "Fortunately there are strategies to mitigate this, here you're going to select a small number of hyperparameters to test a RandomForestClassifier, and use a Randomized Search algorithm with K-Fold Cross Validation to avoid doing a full search across the grid. \n",
    "\n",
    "Remember: take in consideration how much time it took to train a single classifier, and define the number of cross validations folds and iterations of the search accordingly. \n",
    "A recommendation: run the training process, go make yourself a cup of coffee, sit somewhere comfortably and forget about it for a while.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c49a11c7-156f-46bb-8bba-be29d1b2ed1a",
   "metadata": {
    "id": "c49a11c7-156f-46bb-8bba-be29d1b2ed1a"
   },
   "source": [
    "3.7. Use `sklearn.model_selection.RandomizedSearchCV()` to find the best combination of hyperparameters for a Random Forest model. \n",
    "\n",
    "The validation metric used to evaluate the models should be \"roc_auc\" (i.e. `scoring=\"roc_auc\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO Write your code here for training a Random Forest model using Random Search\n",
    "# of hyper-parameters.\n",
    "#   - Please use sklearn.model_selection.RandomizedSearchCV() and\n",
    "#     sklearn.ensemble.RandomForestClassifier() classes.\n",
    "#   - Assign the RandomizedSearchCV model to the variable `rf_random`.\n",
    "#   - Remember to fit the model only on `train_data`.\n",
    "rf_random = \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b09205aa-5afc-481a-b45b-d5e80b56d804",
   "metadata": {
    "id": "b09205aa-5afc-481a-b45b-d5e80b56d804"
   },
   "source": [
    "3.8. Use the classifier to predict probabilities on the train and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429ca94-57f4-487e-b3dd-883bc7bc7835",
   "metadata": {
    "executionInfo": {
     "elapsed": 10144,
     "status": "ok",
     "timestamp": 1670198429448,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "4429ca94-57f4-487e-b3dd-883bc7bc7835"
   },
   "outputs": [],
   "source": [
    "# TODO Use the RandomizedSearchCV model to predict probabilities for each class and\n",
    "# then, use the probabilities for the class 1 only.\n",
    "\n",
    "# Train data predictions (class 1)\n",
    "rf_tuned_pred_train =\n",
    "\n",
    "# Validation data predictions (class 1)\n",
    "rf_tuned_pred_val ="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e645e849-a7e0-43af-b7d1-0b84c29f0e70",
   "metadata": {
    "id": "e645e849-a7e0-43af-b7d1-0b84c29f0e70"
   },
   "source": [
    "3.9. Get AUC ROC score on train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f2204-304a-405b-b51b-350f054eb3f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1670199416165,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "953f2204-304a-405b-b51b-350f054eb3f4",
    "outputId": "e0c4bfdd-1188-4e6f-9bee-0148aa036543"
   },
   "outputs": [],
   "source": [
    "# TODO Get the ROC AUC Score on train_data and val_data datasets.\n",
    "# Train ROC AUC Score\n",
    "roc_auc_train =\n",
    "\n",
    "# Validation ROC AUC Score\n",
    "roc_auc_val ="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "_ZAAkhx1X1Qt",
   "metadata": {
    "id": "_ZAAkhx1X1Qt"
   },
   "source": [
    "At this point, the model should produce a result around 0.7 or higher.\n",
    "\n",
    "**Question:** Comparing train and validation results, do you observe underfitting, overfitting, or none of those two?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2143f7b6",
   "metadata": {},
   "source": [
    "## 4. Predict unlabeled data\n",
    "\n",
    "Now it's time to finally use the `test_data` samples. Because we don't have the labels we can't see how the model performs on this dataset (╯°□°)╯︵ ┻━┻\n",
    "\n",
    "But... don't worry, we will internally evaluate your model and give feedback on the results!\n",
    "\n",
    "In the cells below:\n",
    "- Take your best model\n",
    "- Take `test_data` (i.e. the dataset after doing the preprocessing and feature engineering part)\n",
    "- Run the data through your model and save the predictions on the `TARGET` column in the `app_test` DataFrame (yeah that we've loaded at the very beginning of this notebook).\n",
    "    - `TARGET` column values must be the probabilities for class 1. So remember to use the `predict_proba()` function from your model as we did in the previous sections.\n",
    "- Save the modified version of the DataFrame with the same name it has before (`dataset/application_test_aai.csv`) and don't forget to submit it alongside the rest of this sprint project code\n",
    "- And finally, don't get confused, you shouldn't submit `dataset/application_train_aai.csv`. So please don't upload your solution with this heavy dataset inside.\n",
    "\n",
    "Let's say your best model is called `best_credit_model_ever`, then your code should be exactly this:\n",
    "\n",
    "```python\n",
    "    test_preds = best_credit_model_ever.predict_proba(test_data)[:, 1]\n",
    "    app_test[\"TARGET\"] = test_preds\n",
    "    app_test.to_csv(config.DATASET_TEST, index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO Use your best model and call the predict_proba() on test_data then,\n",
    "# use the probabilities for the class 1 only.\n",
    "# Then, put the predictions in app_test[\"TARGET\"] and save the DataFrame as a csv\n",
    "# with the same name it originally has (\"application_test_aai.csv\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad777cd",
   "metadata": {},
   "source": [
    "## 5. Optional exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e7335-f3cc-410d-81f2-f110f3fbb252",
   "metadata": {
    "id": "d72e7335-f3cc-410d-81f2-f110f3fbb252"
   },
   "source": [
    "### Optional: Training a LightGBM model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d58b7-9f70-4bfb-8b72-20a626e00ea0",
   "metadata": {
    "id": "015d58b7-9f70-4bfb-8b72-20a626e00ea0"
   },
   "source": [
    "5.1. Gradient Boosting Machine is one of the most used machine learning algorithms for tabular data. Lots of competitions have been won using models from libraries like XGBoost or LightGBM. You can try using [LightGBM](https://lightgbm.readthedocs.io/en/latest/) to train a new model an see how it performs compared to the other classifiers you trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d497eb-2b8b-43fe-945e-26a04b8fc004",
   "metadata": {
    "id": "91d497eb-2b8b-43fe-945e-26a04b8fc004"
   },
   "outputs": [],
   "source": [
    "### Complete in this cell: train a LightGBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d1a1f4-5e1e-4982-a6ae-a27b8c11428e",
   "metadata": {
    "id": "c2d1a1f4-5e1e-4982-a6ae-a27b8c11428e"
   },
   "source": [
    "### Optional: Using Scikit Learn Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f95fb-73bf-42c4-97a3-80078f2496aa",
   "metadata": {
    "id": "2e4f95fb-73bf-42c4-97a3-80078f2496aa"
   },
   "source": [
    "5.2. So far you've created special functions or blocks or code to chain operations on data and then train the models. But, reproducibility is important, and you don't want to have to remember the correct steps to follow each time you have new data to train your models. There are a lots of tools out there that can help you with that, here you can use a [Sklearn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to process your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5eecdf-ee08-4ebd-8667-25cdb9a3eef4",
   "metadata": {
    "id": "5a5eecdf-ee08-4ebd-8667-25cdb9a3eef4"
   },
   "outputs": [],
   "source": [
    "### Complete in this cell: use a sklearn Pipeline to automate the cleaning, standardizing and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fcadf8",
   "metadata": {},
   "source": [
    "### Optional: Build your own model and features\n",
    "\n",
    "5.3. If you want you can take the original labeled data given and make your own feature selection, data preprocessing, and model tunning. Be creative, the only limit is time and hardware resources. Only be careful and don't modify the previous functions made in the mandatory assignments or, you will break the project tests.\n",
    "\n",
    "You can even use this newer model to make predictions in the test dataset with hidden labels and submit that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942da44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete in this cell: Make you own experimentation process"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sp02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10 (main, Jan 15 2022, 11:48:00) \n[Clang 13.0.0 (clang-1300.0.29.3)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e94ba2b62eb694bae49fa8de0ed9e62de168312db56a1c4a0a8d614a40cedec1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
